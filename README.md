20191104 Emotion
================

선형 회귀 분석
--------------
- 어떤 변수에 다른 변수들이 주는 영향력을 선형적으로 분석하는 방법
- 선형 귀 분석을 하기 위해서는 우선 선형 회귀 모델을 만들어야 한다. 이때, 모델이란 수학 식으로 표현되는 함수를 의미하는데, 영향을 주는 변수와 영향을 받는 변수로 구성되어 있다.

- 영향을 주는 변수 : 독립 변수 / 설명 변수
- 영향을 받는 변수 : 종속 변수 / 반응 변수

- 선형 회귀 모델을 구성하는 종속 변수와 독립 변수는 각각 2개 이상의 될 수도 있다.
  * 종속 변수가 1개 일 때의 선형 회귀 모델 : 단변량 선형 회귀 모델
  * 종속 변수가 2개 이상일 때의 선형 회귀 모델 : 다변량 선형 회귀 모델
  * 독립 변수가 1개 일 때의 선형 회귀 모델 : 단순 선형 회귀 모데
  * 독렙 변수가 2개 이상일 때의 선형 회귀 모델 : 다중 선형 회귀 모델
  * 종속 변수가 1개이고 독립 변수가 2개 이상인 선형 회귀 모델 : 단변량 다중 선형 회귀 모델

손실 함수
---------
- ANN에서는 학습을 통해 최적의 가중치 매개변수를 결정하기 위한 지표로 손실 함수를 사용한다.
- 손실 함수의 결과값(오차)를 가장 적게 만드는 것이 신경망 학습의 목표이다.
- 손실 함수의 결과값을 작게 만들기 위해서 가중치 매개변수를 조작해 나가는 과정이 학습, 각각의 가중치 매개변수를 어디로 얼마나 조절해야 손실 함수의 결과 값이 적어질지를 결정할 때 참고하는 것이 미분값(기울기)이다.

- 손실 함수로는 보통 두 가지를 주로 사용한다.
1. 평균 제곱 오차 : 회귀에서 항등 함수의 손실 함수로 사용
2. 교차 엔트로피 오차 : 분류에서 소프트맥스 함수의 손실함수로 사용

경사 하강법
-----------
- 경사 하강법 : 손실(cost)를 줄이는 알고리즘(미분 값(기울기)이 최소가 되는 점을 찾아 알맞은 weight(가중치 매개변수)를 찾아내는 것

- 경사 하강법을 찾는 순서
1. w1에 대한 시작점을 선택하는 것
   * 많은 알고리즘에서는 w1를 0으로 설정하거나 임의의 값을 선택한다.
2. 시작점에서 손실 곡선의 기울기를 계산한다.
   * 단일 가중치에 대한 손실의 기울기는 미분 값과 같다.
   * 손실함수 곡선의 다음 지점을 결정하기 위해 경사하강법 알고리즘은 단일 가중의 일부를 시작점에 더한다.
   * 기울기의 보폭을 통해 손실 곡선의 다음 지점으로 이동한다.
3. 경사하강법은 위의 과정을 반복해 최소값에 점점 접근한다.

퍼셉트론
--------
- 퍼셉트론은 수용층, 연합층, 반응층의 세 부분으로 구성되어 있다.
1. 수용층 : 외부 자극을 받아들임
2. 연합층 : 수용층의 가중 압력을 받아 반응층으로 전달하는 기능을 담당
3. 반응층 : 최종 출력을 내보내는 기능

- 퍼셉트론은 연결 강도를 조정하기 위하여 반복적 학습 방법을 사용한다.
- 퍼셉트론은 AND, OR 연산 등 선형 분리가 가능한 문제의 해결에만 사용될 수 있었다.

로지스틱 회귀
-------------
- 로지스틱 회귀 : 독립 변수의 선형 결합을 이용하여 사건의 발생 가능성을 예측하는데 사용되는 통계 기법
- 로지스틱 회귀의 목적은 일반적인 회귀 분석의 목표와 동일하게 종속 변수와 독립 변수간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용하는 것이다. 이는 독립 변수의 선형 결합으로 종속 변수를 설명한다는 관점에서는 선형 회귀 분석과 유사하다.
  * 하지만 로지스틱 회귀는 선형 회귀 분석과는 다르게 종속 변수가 범주형 데이터를 대상으로하며 입력 데이터가 주어졌을 때 해당 데이터의 결과가 특정 분류로 나뉘기 때문에 일종의 분류 기법으로도 볼 수 있다.
- 종속 변수가 이항형 문제(유호한 범주의 개수가 두 개인 경우)를 지칭할 때 사용된다. 이외에, 두 개 이상의 범주를 가지는 문제가 대상인 경우엔 다항 로지스틱 회귀 또는 분화 로지스틱 회귀라고 하고 복수의 범주이면서 순서가 존재하면 서수 로지스틱 회귀라고 한다.
- 로지스틱 회귀 분석은 의료, 통신, 데이터마이닝과 같은 다양한 분야에서 분류 및 예측을 위한 모델로서 폭넓게 사용되고 있다.

시그모이드 함수
---------------
- 시그모이드 함수는 0과 1 사이의 값을 가지는 S 모양의 곡선이다.
- 로지스틱 함수의 특별한 형태이다.
- 다중 퍼셉트론에서 오차 역전파 알고리즘으로 학습하면 입력층에 가까운 쪽으로 갈수록 가중치가 점점 작아져서 결국 0에 가까워지고 학습이 잘 되지 않는 문제가 발생한다. 이와 같은 문제를 VGP라고 한다.
